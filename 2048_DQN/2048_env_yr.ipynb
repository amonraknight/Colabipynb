{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syy0hDteBjI5"
      },
      "source": [
        "This is the demo by Yang Rui on Github.\n",
        "url: https://github.com/YangRui2015/2048_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_ER6uwFQ2xN",
        "outputId": "872a12f0-51ad-4f76-840c-af6ac228a7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwXiUz0GBxx3"
      },
      "source": [
        "#Part 1: utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QBj4gFrMBLeG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "\n",
        "BASE_OUTPUT_PATH = './drive/MyDrive/outputs/'\n",
        "LATEST_MODEL_RECORD_FILE = 'THE_LAST_MODEL'\n",
        "\n",
        "\n",
        "def log2_shaping(s, divide=16):\n",
        "    s = np.log2(1 + s) / divide\n",
        "    return s\n",
        "\n",
        "\n",
        "def check_path_exist(path, verbose=True):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "        if verbose:\n",
        "            print(\"make the dir {} finished\".format(path))\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"the directory {} already exists\".format(path))\n",
        "\n",
        "\n",
        "def running_average(lis, length=5):\n",
        "    if len(lis) > 10:\n",
        "        end = len(lis) // length\n",
        "        lis = lis[:end * length]\n",
        "        arr = np.array(lis).reshape(-1, length)\n",
        "        arr = arr.mean(axis=1)\n",
        "\n",
        "        return list(arr.reshape(-1))\n",
        "    else:\n",
        "        return lis\n",
        "\n",
        "\n",
        "def plot_save(lis, path, title=None, x_label=None, y_label=None):\n",
        "    dir = path.split(\"/\")[:-1]\n",
        "    dir = \"/\".join(dir) + \"/\"\n",
        "    check_path_exist(dir, verbose=False)\n",
        "    plt.figure()\n",
        "    if type(lis[0]) == list:\n",
        "        for li in lis:\n",
        "            plt.plot(li)\n",
        "    else:\n",
        "        plt.plot(lis)\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label)\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.close(\"all\")\n",
        "\n",
        "\n",
        "def del_dir_tree(path):\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            shutil.rmtree(path)\n",
        "        except:\n",
        "            print(\"remove path {} failed!\".format(path))\n",
        "\n",
        "\n",
        "def del_files(path):\n",
        "    if os.path.isdir(path):\n",
        "        files = os.listdir(path)\n",
        "        for file in files:\n",
        "            os.remove(os.path.join(path, file))\n",
        "        print(\"Remove files in {}\".format(path))\n",
        "    elif os.path.isfile(path):\n",
        "        os.remove(path)\n",
        "        print(\"Remove file {}\".format(path))\n",
        "    else:\n",
        "        print(\"{} not a file or a directory\".format(path))\n",
        "\n",
        "\n",
        "class Perfomance_Saver():\n",
        "    '''目前先支持txt'''\n",
        "\n",
        "    def __init__(self, path='performance_data.txt'):\n",
        "        self.path = BASE_OUTPUT_PATH + path\n",
        "        self.clear_file()\n",
        "\n",
        "    def clear_file(self):\n",
        "        if os.path.exists(self.path): \n",
        "          with open(self.path, 'w') as file:\n",
        "              file.write('clear since :{}\\n\\n'.format(time.ctime()))\n",
        "          print(\"clear file finished\")\n",
        "\n",
        "    def save(self, performance_list, info):\n",
        "        with open(self.path, 'a+') as file:\n",
        "            file.writelines(\"time: {}\\n\".format(time.ctime()))\n",
        "            file.writelines(\"info: {} \\n\".format(str(info)))\n",
        "            performance_str = [str(x) + \" \" for x in performance_list]\n",
        "            file.writelines(performance_str)\n",
        "            file.writelines('\\n\\n')\n",
        "        print('write to file finished')\n",
        "\n",
        "\n",
        "class Model_Saver():\n",
        "    '''存一定数量高分模型，防止模型存过多'''\n",
        "\n",
        "    def __init__(self, num=10):\n",
        "        self.num_max = num\n",
        "        self.path_list = []\n",
        "\n",
        "    def save(self, filename):\n",
        "        if len(self.path_list) >= self.num_max:\n",
        "            os.remove(self.path_list.pop(0))\n",
        "            print('del surplus modle files')\n",
        "        path_model = BASE_OUTPUT_PATH + filename\n",
        "        self.path_list.append(path_model)\n",
        "\n",
        "        path_latest_record = BASE_OUTPUT_PATH + LATEST_MODEL_RECORD_FILE\n",
        "        with open(path_latest_record, 'w') as file:\n",
        "            file.writelines(filename)\n",
        "\n",
        "    def read_latest_model_path(self):\n",
        "        path_latest_record = BASE_OUTPUT_PATH + LATEST_MODEL_RECORD_FILE\n",
        "        if os.path.exists(path_latest_record):\n",
        "            with open(path_latest_record, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                if len(lines) >= 1:\n",
        "                    self.path_list.append(BASE_OUTPUT_PATH + lines[len(lines) - 1])\n",
        "                    return lines[len(lines) - 1]\n",
        "                else:\n",
        "                    return None\n",
        "        else:\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX13qXktCZkE"
      },
      "source": [
        "#Part 2: NN_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bCpzD4UcCgUB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# CNN网络\n",
        "class CNN_Net(nn.Module):\n",
        "    def __init__(self, input_len, output_num, conv_size=(32, 64), fc_size=(1024, 128), out_softmax=False):\n",
        "        super(CNN_Net, self).__init__()\n",
        "        self.input_len = input_len\n",
        "        self.output_num = output_num\n",
        "        self.out_softmax = out_softmax \n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, conv_size[0], kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(conv_size[0], conv_size[1], kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(conv_size[1] * self.input_len * self.input_len, fc_size[0])\n",
        "        self.fc2 = nn.Linear(fc_size[0], fc_size[1])\n",
        "        self.head = nn.Linear(fc_size[1], self.output_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1,1,self.input_len, self.input_len)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        output = self.head(x)\n",
        "        if self.out_softmax:\n",
        "            output = F.softmax(output, dim=1)   #值函数估计不应该有softmax\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q-DfnZNVA7z"
      },
      "source": [
        "#Part 3: Buffer_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AC_6KKs4VH7T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class SumTree(object):\n",
        "    data_pointer = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity  # for all priority values\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
        "        #             size: capacity - 1                       size: capacity\n",
        "        self.data = np.zeros(capacity, dtype=object)  # for all transitions，格式是对象，相当于指针\n",
        "        # [--------------data frame-------------]\n",
        "        #             size: capacity\n",
        "\n",
        "    def add(self, p, data):\n",
        "        tree_idx = self.data_pointer + self.capacity - 1  # 在树的叶子节点的位置\n",
        "        self.data[self.data_pointer] = data  # update data_frame\n",
        "        self.update(tree_idx, p)  # update tree_frame\n",
        "\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_idx, p):\n",
        "        change = p - self.tree[tree_idx]\n",
        "        self.tree[tree_idx] = p\n",
        "        # then propagate the change through tree\n",
        "        while tree_idx != 0:  # this method is faster than the recursive loop in the reference code\n",
        "            tree_idx = (tree_idx - 1) // 2\n",
        "            self.tree[tree_idx] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"\n",
        "        Tree structure and array storage:\n",
        "\n",
        "        Tree index:\n",
        "             0         -> storing priority sum\n",
        "            / \\\n",
        "          1     2\n",
        "         / \\   / \\\n",
        "        3   4 5   6    -> storing priority for transitions\n",
        "\n",
        "        Array type for storing:\n",
        "        [0,1,2,3,4,5,6]\n",
        "        \"\"\"\n",
        "        parent_idx = 0\n",
        "        while True:  # the while loop is faster than the method in the reference code\n",
        "            cl_idx = 2 * parent_idx + 1  # this leaf's left and right kids\n",
        "            cr_idx = cl_idx + 1\n",
        "            if cl_idx >= len(self.tree):  # reach bottom, end search\n",
        "                leaf_idx = parent_idx\n",
        "                break\n",
        "            else:  # downward search, always search for a higher priority node\n",
        "                if v <= self.tree[cl_idx]:\n",
        "                    parent_idx = cl_idx\n",
        "                else:\n",
        "                    v -= self.tree[cl_idx]\n",
        "                    parent_idx = cr_idx\n",
        "\n",
        "        data_idx = leaf_idx - self.capacity + 1\n",
        "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
        "\n",
        "    @property\n",
        "    def total_p(self):\n",
        "        return self.tree[0]  # the root\n",
        "\n",
        "class Buffer_PER(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    epsilon = 0.01  # small amount to avoid zero priority\n",
        "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
        "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
        "    beta_increment_per_sampling = 0.001\n",
        "    abs_err_upper = 1.  # clipped abs error\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "\n",
        "    def store(self, transition):\n",
        "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
        "        if max_p == 0:\n",
        "            max_p = self.abs_err_upper\n",
        "        self.tree.add(max_p, transition)  # set the max p for new p\n",
        "\n",
        "    def sample(self, n):\n",
        "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty(\n",
        "            (n, 1))\n",
        "        pri_seg = self.tree.total_p / n  # priority segment\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
        "\n",
        "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p  # for later calculate ISweight\n",
        "        for i in range(n):\n",
        "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
        "            v = np.random.uniform(a, b)\n",
        "            idx, p, data = self.tree.get_leaf(v)\n",
        "            prob = p / self.tree.total_p\n",
        "            ISWeights[i, 0] = np.power(prob / min_prob, -self.beta)\n",
        "            b_idx[i], b_memory[i, :] = idx, data\n",
        "\n",
        "        return b_idx, b_memory, ISWeights\n",
        "\n",
        "    def batch_update(self, tree_idx, abs_errors):\n",
        "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
        "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
        "        ps = np.power(clipped_errors, self.alpha)\n",
        "        for ti, p in zip(tree_idx, ps):\n",
        "            self.tree.update(ti, p)\n",
        "\n",
        "class Buffer():\n",
        "  def __init__(self, n_features, buffer_type='', capacity=1e4):\n",
        "      self.memory_size = capacity\n",
        "      self.n_features = n_features\n",
        "      self.type = buffer_type\n",
        "      self.memory_counter = 0\n",
        "\n",
        "      if self.type == 'priority':\n",
        "          self.memory = Buffer_PER(capacity=capacity)\n",
        "      else:\n",
        "          self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
        "\n",
        "  def store(self, transition):\n",
        "      self.memory_counter += 1\n",
        "\n",
        "      if self.type == 'priority':\n",
        "          self.memory.store(transition)\n",
        "      else:\n",
        "          index = self.memory_counter % self.memory_size\n",
        "          self.memory[index, :] = transition\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "      info = None\n",
        "      if self.type == 'priority':\n",
        "          tree_idx, batch_memory, ISWeights = self.memory.sample(batch_size)\n",
        "          info = (tree_idx, ISWeights)\n",
        "      else:\n",
        "          sample_index = np.random.choice(self.memory_size, size=batch_size)  # 考虑buffer已先填满\n",
        "          batch_memory = self.memory[sample_index, :]\n",
        "\n",
        "      return batch_memory, info\n",
        "\n",
        "  def update(self, tree_idx, td_errors):\n",
        "      assert self.type == 'priority'\n",
        "      self.memory.batch_update(tree_idx, td_errors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuYlfDZjVmnn"
      },
      "source": [
        "#Part 4: dqn_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aMH5VMU0WfdV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DQN():\n",
        "    batch_size = 128\n",
        "    lr = 1e-4\n",
        "    epsilon = 0.15   \n",
        "    memory_capacity = int(1e4)\n",
        "    gamma = 0.99\n",
        "    q_network_iteration = 200\n",
        "    save_path = \"./drive/MyDrive/outputs/\"\n",
        "    soft_update_theta = 0.1\n",
        "    clip_norm_max = 1\n",
        "    train_interval = 5\n",
        "    conv_size = (32, 64)   # num filters\n",
        "    fc_size = (512, 128)\n",
        "\n",
        "    def __init__(self, num_state, num_action, enable_double=True, enable_priority=True):\n",
        "        super(DQN, self).__init__()\n",
        "        self.num_state = num_state\n",
        "        self.num_action = num_action\n",
        "        self.state_len = int(np.sqrt(self.num_state))\n",
        "        self.enable_double = enable_double\n",
        "        self.enable_priority = enable_priority\n",
        "\n",
        "        self.eval_net, self.target_net = CNN_Net(self.state_len, num_action,self.conv_size, self.fc_size), CNN_Net(self.state_len, num_action, self.conv_size, self.fc_size)\n",
        "\n",
        "        self.learn_step_counter = 0\n",
        "        self.buffer = Buffer(self.num_state, 'priority', self.memory_capacity)  \n",
        "        self.initial_epsilon = self.epsilon\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.lr)\n",
        "\n",
        "    def select_action(self, state, random=False, deterministic=False):\n",
        "        state = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
        "        if not random and np.random.random() > self.epsilon or deterministic:  # greedy policy\n",
        "            action_value = self.eval_net.forward(state)\n",
        "            action = torch.max(action_value.reshape(-1, 4), 1)[1].data.numpy()\n",
        "        else:  # random policy\n",
        "            action = np.random.randint(0, self.num_action)\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state):\n",
        "        state = state.reshape(-1)\n",
        "        next_state = next_state.reshape(-1)\n",
        "\n",
        "        transition = np.hstack((state, [action, reward], next_state))\n",
        "        self.buffer.store(transition)\n",
        "\n",
        "    def save(self, path=None, name='dqn_net.pkl'):\n",
        "        path = self.save_path if not path else path\n",
        "        check_path_exist(path)\n",
        "        torch.save(self.eval_net.state_dict(), path + name)\n",
        "\n",
        "    def load(self, path=None, name='dqn_net.pkl'):\n",
        "        path = self.save_path if not path else path\n",
        "        self.eval_net.load_state_dict(torch.load(path + name))\n",
        "\n",
        "    def epsilon_decay(self, episode, total_episode):\n",
        "        self.epsilon = self.initial_epsilon * (1 - episode / total_episode)\n",
        "\n",
        "    def update(self):\n",
        "        # soft update the parameters\n",
        "        if self.learn_step_counter % self.q_network_iteration == 0 and self.learn_step_counter:\n",
        "            for p_e, p_t in zip(self.eval_net.parameters(), self.target_net.parameters()):\n",
        "                p_t.data = self.soft_update_theta * p_e.data + (1 - self.soft_update_theta) * p_t.data\n",
        "\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "        # sample batch from memory\n",
        "        if self.enable_priority:\n",
        "            batch_memory, (tree_idx, ISWeights) = self.buffer.sample(self.batch_size)\n",
        "        else:\n",
        "            batch_memory, _ = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        batch_state = torch.FloatTensor(batch_memory[:, :self.num_state])\n",
        "        batch_action = torch.LongTensor(batch_memory[:, self.num_state: self.num_state + 1].astype(int))\n",
        "        batch_reward = torch.FloatTensor(batch_memory[:, self.num_state + 1: self.num_state + 2])\n",
        "        batch_next_state = torch.FloatTensor(batch_memory[:, -self.num_state:])\n",
        "\n",
        "        # q_eval\n",
        "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
        "        q_eval_next = self.eval_net(batch_next_state)\n",
        "        q_target_next = self.target_net(batch_next_state).detach()\n",
        "\n",
        "        if self.enable_double:\n",
        "            q_eval_argmax = q_eval_next.max(1)[1].view(self.batch_size, 1)\n",
        "            q_max = q_target_next.gather(1, q_eval_argmax).view(self.batch_size, 1)\n",
        "        else:\n",
        "            q_max = q_target_next.max(1)[0].view(self.batch_size, 1)\n",
        "        q_target = batch_reward + self.gamma * q_max\n",
        "\n",
        "        if self.enable_priority:\n",
        "            abs_errors = (q_target - q_eval.data).abs()\n",
        "            self.buffer.update(tree_idx, abs_errors)\n",
        "            loss = (torch.FloatTensor(ISWeights) * (q_target - q_eval).pow(2)).mean()  # with importance sampling weight\n",
        "            # loss = (q_target - q_eval).pow(2).mean()  # without importance sampling weight\n",
        "        else:\n",
        "            loss = F.mse_loss(q_eval, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_net.parameters(), self.clip_norm_max)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqcxHmN0X_uV"
      },
      "source": [
        "#Part 5: gym_2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zLnqQDucYHoh"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "import itertools\n",
        "import logging\n",
        "from six import StringIO\n",
        "import sys\n",
        "\n",
        "\n",
        "def pairwise(iterable):\n",
        "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
        "    a, b = itertools.tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "\n",
        "\n",
        "class IllegalMove(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def stack(flat, layers=16):\n",
        "    larray = []\n",
        "    for i in range(1, layers + 1):\n",
        "        ii = 2 ** i\n",
        "        layer = np.copy(flat)\n",
        "        layer[layer != ii] = 0\n",
        "        layer[layer == ii] = 1\n",
        "        # print(\"Layer\")\n",
        "        # print(layer)\n",
        "        # print(layer.shape)\n",
        "        larray.append(layer)\n",
        "\n",
        "    newstack = np.stack(larray, axis=-1)\n",
        "    return newstack\n",
        "\n",
        "\n",
        "class Game2048Env(gym.Env):  # directions 0, 1, 2, 3 are up, right, down, left\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "    max_steps = 10000\n",
        "\n",
        "    def __init__(self):\n",
        "        # Definitions for game. Board must be square.\n",
        "        self.size = 4\n",
        "        self.w = self.size\n",
        "        self.h = self.size\n",
        "        self.squares = self.size * self.size\n",
        "\n",
        "        # Maintain own idea of game score, separate from rewards\n",
        "        self.score = 0\n",
        "\n",
        "        # Members for gym implementation\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        # Suppose that the maximum tile is as if you have powers of 2 across the board.\n",
        "        layers = self.squares\n",
        "        self.observation_space = spaces.Box(0, 1, (self.w, self.h, layers), dtype=np.int)\n",
        "        self.set_illegal_move_reward(0.)\n",
        "        self.set_max_tile(None)\n",
        "\n",
        "        self.max_illegal = 10  # max number of illegal actions\n",
        "        self.num_illegal = 0\n",
        "\n",
        "        # Initialise seed\n",
        "        self.seed()\n",
        "\n",
        "        # # Reset ready for a game\n",
        "        # self.reset()\n",
        "\n",
        "    def _get_info(self, info=None):\n",
        "        if not info:\n",
        "            info = {}\n",
        "        else:\n",
        "            assert type(info) == dict, 'info should be of type dict!'\n",
        "\n",
        "        info['highest'] = self.highest()\n",
        "        info['score'] = self.score\n",
        "        info['steps'] = self.steps\n",
        "        return info\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def set_illegal_move_reward(self, reward):\n",
        "        \"\"\"Define the reward/penalty for performing an illegal move. Also need\n",
        "            to update the reward range for this.\"\"\"\n",
        "        # Guess that the maximum reward is also 2**squares though you'll probably never get that.\n",
        "        # (assume that illegal move reward is the lowest value that can be returned\n",
        "        self.illegal_move_reward = reward\n",
        "        self.reward_range = (self.illegal_move_reward, float(2 ** self.squares))\n",
        "\n",
        "    def set_max_tile(self, max_tile):\n",
        "        \"\"\"Define the maximum tile that will end the game (e.g. 2048). None means no limit.\n",
        "           This does not affect the state returned.\"\"\"\n",
        "        assert max_tile is None or isinstance(max_tile, int)\n",
        "        self.max_tile = max_tile\n",
        "\n",
        "    # Implement gym interface\n",
        "    def step(self, action):\n",
        "        \"\"\"Perform one step of the game. This involves moving and adding a new tile.\"\"\"\n",
        "        logging.debug(\"Action {}\".format(action))\n",
        "        self.steps += 1\n",
        "        score = 0\n",
        "        done = None\n",
        "        info = {\n",
        "            'illegal_move': False,\n",
        "        }\n",
        "        try:\n",
        "            score = float(self.move(action))\n",
        "            self.score += score\n",
        "            assert score <= 2 ** (self.w * self.h)\n",
        "            self.add_tile()\n",
        "            done = self.isend()\n",
        "            reward = float(score)\n",
        "        except IllegalMove as e:\n",
        "            logging.debug(\"Illegal move\")\n",
        "            info['illegal_move'] = True\n",
        "            if self.steps > self.max_steps:\n",
        "                done = True\n",
        "            else:\n",
        "                done = False\n",
        "            reward = self.illegal_move_reward\n",
        "            self.num_illegal += 1\n",
        "            if self.num_illegal >= self.max_illegal:  # exceed the maximum number of illegal actions\n",
        "                done = True\n",
        "\n",
        "        info = self._get_info(info)\n",
        "\n",
        "        # Return observation (board state), reward, done and info dict\n",
        "        return self.Matrix, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.Matrix = np.zeros((self.h, self.w), np.int)\n",
        "        self.score = 0\n",
        "        self.steps = 0\n",
        "        self.num_illegal = 0\n",
        "\n",
        "        logging.debug(\"Adding tiles\")\n",
        "        self.add_tile()\n",
        "        self.add_tile()\n",
        "\n",
        "        return self.Matrix, 0, False, self._get_info()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "        s = 'Score: {}\\n'.format(self.score)\n",
        "        s += 'Highest: {}\\n'.format(self.highest())\n",
        "        npa = np.array(self.Matrix)\n",
        "        grid = npa.reshape((self.size, self.size))\n",
        "        s += \"{}\\n\\n\".format(grid)\n",
        "        outfile.write(s)\n",
        "        return outfile\n",
        "\n",
        "    # Implement 2048 game\n",
        "    def add_tile(self):\n",
        "        \"\"\"Add a tile, probably a 2 but maybe a 4\"\"\"\n",
        "        possible_tiles = np.array([2, 4])\n",
        "        tile_probabilities = np.array([0.9, 0.1])\n",
        "        val = self.np_random.choice(possible_tiles, 1, p=tile_probabilities)[0]\n",
        "        empties = self.empties()\n",
        "        assert empties.shape[0]\n",
        "        empty_idx = self.np_random.choice(empties.shape[0])\n",
        "        empty = empties[empty_idx]\n",
        "        logging.debug(\"Adding %s at %s\", val, (empty[0], empty[1]))\n",
        "        self.set(empty[0], empty[1], val)\n",
        "\n",
        "    def get(self, x, y):\n",
        "        \"\"\"Return the value of one square.\"\"\"\n",
        "        return self.Matrix[x, y]\n",
        "\n",
        "    def set(self, x, y, val):\n",
        "        \"\"\"Set the value of one square.\"\"\"\n",
        "        self.Matrix[x, y] = val\n",
        "\n",
        "    def empties(self):\n",
        "        \"\"\"Return a 2d numpy array with the location of empty squares.\"\"\"\n",
        "        return np.argwhere(self.Matrix == 0)\n",
        "\n",
        "    def highest(self):\n",
        "        \"\"\"Report the highest tile on the board.\"\"\"\n",
        "        return np.max(self.Matrix)\n",
        "\n",
        "    def move(self, direction, trial=False):\n",
        "        \"\"\"Perform one move of the game. Shift things to one side then,\n",
        "        combine. directions 0, 1, 2, 3 are up, right, down, left.\n",
        "        Returns the score that [would have] got.\"\"\"\n",
        "        if not trial:\n",
        "            if direction == 0:\n",
        "                logging.debug(\"Up\")\n",
        "            elif direction == 1:\n",
        "                logging.debug(\"Right\")\n",
        "            elif direction == 2:\n",
        "                logging.debug(\"Down\")\n",
        "            elif direction == 3:\n",
        "                logging.debug(\"Left\")\n",
        "\n",
        "        changed = False\n",
        "        move_score = 0\n",
        "        dir_div_two = int(direction / 2)\n",
        "        dir_mod_two = int(direction % 2)\n",
        "        shift_direction = dir_mod_two ^ dir_div_two  # 0 for towards up left, 1 for towards bottom right\n",
        "\n",
        "        # Construct a range for extracting row/column into a list\n",
        "        rx = list(range(self.w))\n",
        "        ry = list(range(self.h))\n",
        "\n",
        "        if dir_mod_two == 0:\n",
        "            # Up or down, split into columns\n",
        "            for y in range(self.h):\n",
        "                old = [self.get(x, y) for x in rx]\n",
        "                (new, ms) = self.shift(old, shift_direction)\n",
        "                move_score += ms\n",
        "                if old != new:\n",
        "                    changed = True\n",
        "                    if not trial:\n",
        "                        for x in rx:\n",
        "                            self.set(x, y, new[x])\n",
        "        else:\n",
        "            # Left or right, split into rows\n",
        "            for x in range(self.w):\n",
        "                old = [self.get(x, y) for y in ry]\n",
        "                (new, ms) = self.shift(old, shift_direction)\n",
        "                move_score += ms\n",
        "                if old != new:\n",
        "                    changed = True\n",
        "                    if not trial:\n",
        "                        for y in ry:\n",
        "                            self.set(x, y, new[y])\n",
        "        if changed != True:\n",
        "            raise IllegalMove\n",
        "\n",
        "        return move_score\n",
        "\n",
        "    def combine(self, shifted_row):\n",
        "        \"\"\"Combine same tiles when moving to one side. This function always\n",
        "           shifts towards the left. Also count the score of combined tiles.\"\"\"\n",
        "        move_score = 0\n",
        "        combined_row = [0] * self.size\n",
        "        skip = False\n",
        "        output_index = 0\n",
        "        for p in pairwise(shifted_row):\n",
        "            if skip:\n",
        "                skip = False\n",
        "                continue\n",
        "            combined_row[output_index] = p[0]\n",
        "            if p[0] == p[1]:\n",
        "                combined_row[output_index] += p[1]\n",
        "                move_score += p[0] + p[1]\n",
        "                # Skip the next thing in the list.\n",
        "                skip = True\n",
        "            output_index += 1\n",
        "        if shifted_row and not skip:\n",
        "            combined_row[output_index] = shifted_row[-1]\n",
        "\n",
        "        return (combined_row, move_score)\n",
        "\n",
        "    def shift(self, row, direction):\n",
        "        \"\"\"Shift one row left (direction == 0) or right (direction == 1), combining if required.\"\"\"\n",
        "        length = len(row)\n",
        "        assert length == self.size\n",
        "        assert direction == 0 or direction == 1\n",
        "\n",
        "        # Shift all non-zero digits up\n",
        "        shifted_row = [i for i in row if i != 0]\n",
        "\n",
        "        # Reverse list to handle shifting to the right\n",
        "        if direction:\n",
        "            shifted_row.reverse()\n",
        "\n",
        "        (combined_row, move_score) = self.combine(shifted_row)\n",
        "\n",
        "        # Reverse list to handle shifting to the right\n",
        "        if direction:\n",
        "            combined_row.reverse()\n",
        "\n",
        "        assert len(combined_row) == self.size\n",
        "        return (combined_row, move_score)\n",
        "\n",
        "    def isend(self):\n",
        "        \"\"\"Has the game ended. Game ends if there is a tile equal to the limit\n",
        "           or there are no legal moves. If there are empty spaces then there\n",
        "           must be legal moves.\"\"\"\n",
        "\n",
        "        if self.max_tile is not None and self.highest() == self.max_tile:\n",
        "            return True\n",
        "\n",
        "        if self.steps >= self.max_steps:\n",
        "            return True\n",
        "\n",
        "        for direction in range(4):\n",
        "            try:\n",
        "                self.move(direction, trial=True)\n",
        "                # Not the end if we can do any move\n",
        "                return False\n",
        "            except IllegalMove:\n",
        "                pass\n",
        "        return True\n",
        "\n",
        "    def get_board(self):\n",
        "        \"\"\"Retrieve the whole board, useful for testing.\"\"\"\n",
        "        return self.Matrix\n",
        "\n",
        "    def set_board(self, new_board):\n",
        "        \"\"\"Retrieve the whole board, useful for testing.\"\"\"\n",
        "        self.Matrix = new_board\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPIi5zUuays5"
      },
      "source": [
        "#Part 6: main_dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHsEMHZWa3m6",
        "outputId": "17c3a3fa-f204-4caf-ea92-8ba92ace596f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clear file finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  ary = asanyarray(ary)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean score:1450.56, mean highest:146.72\n",
            "max score:3060.0, max hightest:256\n",
            "write to file finished\n",
            "the directory ./drive/MyDrive/outputs/ already exists\n",
            "episolde19375 eval average score 1450.56, eval max socre 3060.0, current time: 2022-11-09 01:01:50.070216\n",
            "mean score:1656.88, mean highest:165.92\n",
            "max score:3188.0, max hightest:256\n",
            "write to file finished\n",
            "the directory ./drive/MyDrive/outputs/ already exists\n",
            "episolde19400 eval average score 1656.88, eval max socre 3188.0, current time: 2022-11-09 01:01:59.945048\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "train_episodes = 50000\n",
        "test_episodes = 50\n",
        "ifrender = False\n",
        "ifprinteachepisode = False\n",
        "ifevaluate = True\n",
        "eval_interval = 25\n",
        "epsilon_decay_interval = 100\n",
        "log_interval = 5\n",
        "backup_interval = 100\n",
        "\n",
        "\n",
        "def train():\n",
        "    episodes = train_episodes\n",
        "    agent = DQN(num_state=16, num_action=4)\n",
        "    env = Game2048Env()\n",
        "\n",
        "    pf_saver = Perfomance_Saver()\n",
        "    model_saver = Model_Saver(num=10)\n",
        "\n",
        "    # If there is already a model, use it\n",
        "    starting_episode = 0\n",
        "    existing_model = model_saver.read_latest_model_path()\n",
        "    if existing_model:\n",
        "        matches = re.findall(r'\\d+', existing_model)\n",
        "        starting_episode = int(matches[0])\n",
        "        agent.load(name=existing_model)\n",
        "\n",
        "    eval_max_score = 0\n",
        "    for i in range(starting_episode+1, episodes):\n",
        "        state, reward, done, info = env.reset()\n",
        "        state = log2_shaping(state)\n",
        "\n",
        "        # start = time.time()\n",
        "        loss = None\n",
        "        while True:\n",
        "            if ifrender:\n",
        "                env.render()\n",
        "\n",
        "            if agent.buffer.memory_counter <= agent.memory_capacity and starting_episode==0:\n",
        "                action = agent.select_action(state, random=True)\n",
        "            else:\n",
        "                action = agent.select_action(state)\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = log2_shaping(next_state)\n",
        "            reward = log2_shaping(reward, divide=1)\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "\n",
        "            if agent.buffer.memory_counter % agent.train_interval == 0 and agent.buffer.memory_counter > agent.memory_capacity:  # 相当于填满后才update\n",
        "                loss = agent.update()\n",
        "\n",
        "            if done:\n",
        "                if i % log_interval == 0:\n",
        "                    \n",
        "                    if loss and ifprinteachepisode:\n",
        "                        print(\n",
        "                            'loss {0}, training progress {1}, episode reward {2}, episode steps {3}, highest {4}, epsilon {5}'.format(\n",
        "                                loss, (i + 1) / episodes, info['score'], info['steps'], info['highest'], agent.epsilon))\n",
        "\n",
        "                    loss = None\n",
        "\n",
        "                if i % epsilon_decay_interval == 0:  # episilon decay\n",
        "                    agent.epsilon_decay(i, episodes)\n",
        "                break\n",
        "\n",
        "        # end = time.time()\n",
        "        # print('episode start time:{} end time: {} s\\n'.format(start, end))\n",
        "\n",
        "        # eval \n",
        "        if ifevaluate and i % eval_interval == 0 and i:\n",
        "            eval_info = test(episodes=test_episodes, agent=agent)\n",
        "            average_score, max_score, score_lis = eval_info['mean'], eval_info['max'], eval_info['list']\n",
        "\n",
        "            pf_saver.save(score_lis, info=f'episode:{i}')\n",
        "\n",
        "            # Save the highest score and every 10 episode.\n",
        "            if i % backup_interval == 0 or int(average_score) > eval_max_score:\n",
        "                name = 'dqn_{}.pkl'.format(i)\n",
        "                agent.save(name=name)\n",
        "                model_saver.save(name)\n",
        "            \n",
        "            datetime_object = datetime.datetime.now()\n",
        "            print(\n",
        "                'episolde{0} eval average score {1}, eval max socre {2}, current time: {3}'.format(\n",
        "                 i, average_score, max_score, datetime_object))\n",
        "\n",
        "\n",
        "def test(episodes=20, agent=None, load_path=None, ifrender=False, log=False):\n",
        "    if agent is None:\n",
        "        agent = DQN(num_state=16, num_action=4)\n",
        "        if load_path:\n",
        "            agent.load(load_path)\n",
        "\n",
        "    env = Game2048Env()\n",
        "    score_list = []\n",
        "    highest_list = []\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _, done, info = env.reset()\n",
        "        state = log2_shaping(state)\n",
        "\n",
        "        start = time.time()\n",
        "        while True:\n",
        "            action = agent.select_action(state, deterministic=True)\n",
        "            next_state, _, done, info = env.step(action)\n",
        "            next_state = log2_shaping(next_state)\n",
        "            state = next_state\n",
        "\n",
        "            if ifrender:\n",
        "                env.render()\n",
        "\n",
        "            if done:\n",
        "                if log:\n",
        "                    print(\n",
        "                        'episode number {0}, episode reward {1}, episode steps {2}, highest {3}'.format(\n",
        "                            i + 1, info['score'], info['steps'], info['highest']))\n",
        "                break\n",
        "\n",
        "        end = time.time()\n",
        "        if log:\n",
        "            print('episode time:{} s\\n'.format(end - start))\n",
        "\n",
        "        score_list.append(info['score'])\n",
        "        highest_list.append(info['highest'])\n",
        "\n",
        "    print('mean score:{}, mean highest:{}'.format(np.mean(score_list), np.mean(highest_list)))\n",
        "    print('max score:{}, max hightest:{}'.format(np.max(score_list), np.max(highest_list)))\n",
        "    result_info = {'mean': np.mean(score_list), 'max': np.max(score_list), 'list': score_list}\n",
        "    return result_info\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # test(episodes=test_episodes, ifrender=ifrender)\n",
        "    train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}